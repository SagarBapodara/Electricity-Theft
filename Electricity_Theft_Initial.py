# -*- coding: utf-8 -*-
"""CS_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k7plUC1tfUsV3WBu6Cc32YQoWVuYUqPt

# Author :  Sagar Bapodara

## Data :

- https://drive.google.com/file/d/1QqJ2KUdSo7vNDc2zM3dHYHRV81SRRs43/view?usp=sharing
"""

# linking google colab and google drive
from google.colab import drive
drive.mount('/content/gdrive')

!ls '/content/gdrive/My Drive/CSE_Project'

"""# Importing Dependencies"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
print('Dependencies Imported')

"""# Loading the dataset"""

data = pd.read_csv('/content/gdrive/My Drive/CSE_Project/data.csv')
data.head(10)

"""# Data Analysis"""

data.shape #42372 rows with 1036 columns

data.info()

data.describe() #without any preprocessing

data.isnull().sum()

data['FLAG'].value_counts()

"""*Information from dataset given : **1** = indicates electricity theft; **0** = indicates normal usage*"""

data.columns

"""# Data Preprocessing"""

org_data = pd.read_csv('/content/gdrive/My Drive/CSE_Project/data.csv')

org_data.head(5)

"""For data visualization, splitting the labels and data columns"""

label_data = pd.DataFrame()
label_data['FLAG'] = org_data['FLAG']
label_data['CONS_NO'] = org_data['CONS_NO']

label_data.head(5)

data = org_data.drop(['FLAG', 'CONS_NO'], axis=1)

data.head(5)

dropIndex = data[data.duplicated()].index  # dropping duplicate rows from data 
data = data.drop(dropIndex, axis=0)

label_data = label_data.drop(dropIndex, axis=0) # dropping duplicate rows from labels

zeroIndex = data[(data.sum(axis=1) == 0)].index  # dropping zero rows from data 
data = data.drop(zeroIndex, axis=0)

label_data = label_data.drop(zeroIndex, axis=0) # dropping zero rows from labels

data.columns = pd.to_datetime(data.columns)  # columns reindexing according to dates
data = data.reindex(sorted(data.columns), axis=1)
cols = data.columns

data.reset_index(inplace=True, drop=True)  # index sorting
label_data.reset_index(inplace=True, drop=True)

"""After Basic Data Processing, """

data.shape

label_data.shape

data = data.interpolate(method='linear', limit=2, limit_direction='both', axis=0).fillna(0) # filling NaN values

data.head(5)

from scipy import stats
z=np.abs(stats.zscore(data))

data=data[(z<3).all(axis=1)] #removing all the values greater than 3 standard deviations

data.to_csv(r'data_for_visualization.csv', index=False, header=True)

rawDatax = pd.read_csv('data_for_visualization.csv')

rawDatax.shape

rawData1 = pd.read_csv('data_for_visualization.csv', nrows=3) #taking first 3 rows
cols = rawData1.columns
rawData2 = pd.read_csv('data_for_visualization.csv', skiprows=35520) #randomly choosing any value from normal consumer
rawData2.columns = cols
data = pd.concat([rawData1, rawData2], ignore_index=True)

rawData1

rawData2

data

"""# Data Visualization"""

data.head(5)

"""Using Z Score"""

#plot 1D graph for consumer
fig, axs = plt.subplots(2, 1)
fig.suptitle('Consumers With Fraud', fontsize=18)
plt.subplots_adjust(hspace=0.8,wspace=0.8)

data.loc[0].plot(ax=axs[0], color='red', grid=True)
axs[0].set_title('Consumer 0', fontsize=13)
axs[0].set_xlabel('Consumption Dates')
axs[0].set_ylabel('Consumption Units')

data.loc[2].plot(ax=axs[1], color='red', grid=True)
axs[1].set_title('Consumer 1', fontsize=13)
axs[1].set_xlabel('Consumption Dates')
axs[1].set_ylabel('Consumption Units')

fig, axs = plt.subplots(2, 1)
fig.suptitle('Consumers Without Fraud', fontsize=13)
plt.subplots_adjust(hspace=0.8)

data.loc[3].plot(ax=axs[0], color='green', grid=True)
axs[0].set_title('Consumer 35521', fontsize=13)
axs[0].set_xlabel('Consumption Dates')
axs[0].set_ylabel('Consumption Units')

data.loc[10].plot(ax=axs[1], color='green', grid=True)
axs[1].set_title('Consumer 35531', fontsize=13)
axs[1].set_xlabel('Consumption Dates')
axs[1].set_ylabel('Consumption Units')

fig2, axs2 = plt.subplots(2, 2)
fig2.suptitle('Statistics for Consumers with Fraud', fontsize=16)
plt.subplots_adjust(hspace=0.8,wspace=0.5)

#consumption chart
data.loc[0].plot(ax=axs2[0, 0], color='red', grid=True)
axs2[0, 0].set_xlabel('Consumption Dates')
axs2[0, 0].set_ylabel('Consumption Units')

#histogram
data.loc[0].hist(color='red', ax=axs2[0, 1], grid=True)
axs2[0, 1].set_title('Histogram', fontsize=13)
axs2[0, 1].set_xlabel('Values')
axs2[0, 1].set_ylabel('Frequency')

#density estimation
data.loc[0].plot.kde(color='red', ax=axs2[1, 0], grid=True)
axs2[1, 0].set_title('Density Estimation', fontsize=13)
axs2[1, 0].set_xlabel('Values')
axs2[1, 0].set_ylabel('Density')

#statistics
data.loc[0].describe().drop(['count']).plot(kind='bar', ax=axs2[1, 1], color='red', grid=True)
axs2[1, 1].set_title('Statistics', fontsize=13)
axs2[1, 1].set_ylabel('Values')

fig3, axs3 = plt.subplots(2, 2)
fig3.suptitle('Statistics for Consumers without Fraud', fontsize=16)
plt.subplots_adjust(hspace=0.8,wspace=0.5)

#consumption chart
data.loc[10].plot(ax=axs3[0, 0], color='green', grid=True)
axs3[0, 0].set_xlabel('Consumption Dates')
axs3[0, 0].set_ylabel('Consumption Units')

#histogram
data.loc[10].hist(color='green', ax=axs3[0, 1])
axs3[0, 1].set_title('Histogram', fontsize=13)
axs3[0, 1].set_xlabel('Values')
axs3[0, 1].set_ylabel('Frequency')

#density estimation
data.loc[10].plot.kde(color='green', ax=axs3[1, 0], grid=True)
axs3[1, 0].set_title('Density Estimation', fontsize=13)
axs3[1, 0].set_xlabel('Values')
axs3[1, 0].set_ylabel('Density')

#statistics
data.loc[10].describe().drop(['count']).plot(kind='bar', ax=axs3[1, 1], color='green', grid=True)
axs3[1, 1].set_title('Statistics', fontsize=13)
axs3[1, 1].set_ylabel('Values')

"""# Data Processing for ML Models"""

data = pd.read_csv('/content/gdrive/My Drive/CSE_Project/ml_data_final.csv')

data.head(10)

data.shape



Y = data['OutPut']
data.drop(['OutPut'],axis=1,inplace=True)

Y

x = np.array(data)
y = np.array(Y)

print(x.shape)
print(y.shape)

"""# Splitting Data into Training *(80%)* | Validation *(10%)* | Test Set *(10%)*"""

from sklearn.model_selection import train_test_split
X_train, X_rem, y_train, y_rem = train_test_split(x,y, train_size=0.8) #80% to be used for training and 20# remaining

#for validation and test set 
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) #10% validation set and 10% testing set

print(X_train.shape), print(y_train.shape)
print(X_valid.shape), print(y_valid.shape)
print(X_test.shape), print(y_test.shape)

"""# Machine Learning Models :

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
Logistic_Reg = LogisticRegression(random_state = 0,max_iter=10000)
Logistic_Reg.fit(X_train, y_train)

"""### Validation Data"""

y_pred_lr_valid = Logistic_Reg.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_lr_valid))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_valid, y_pred_lr_valid)

"""### Testing Data"""

y_pred_lr_test = Logistic_Reg.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_lr_test))

confusion_matrix(y_test, y_pred_lr_test)

"""## KNN

### N = 3 | Validation
"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, y_train)

y_pred_knn_valid = classifier.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_knn_valid))

confusion_matrix(y_valid, y_pred_knn_valid)

"""### N = 5 | Validation"""

classifier_2 = KNeighborsClassifier(n_neighbors=5)
classifier_2.fit(X_train, y_train)

y_pred_knn_valid_2 = classifier_2.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_knn_valid_2))

confusion_matrix(y_valid, y_pred_knn_valid_2)

"""### N = 10 | Validation"""

classifier_3 = KNeighborsClassifier(n_neighbors=10)
classifier_3.fit(X_train, y_train)

y_pred_knn_valid_3 = classifier_3.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_knn_valid_3))

confusion_matrix(y_valid, y_pred_knn_valid_3)

"""### N = 10 | Testing Data"""

y_pred_knn_test = classifier_3.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_knn_test))

confusion_matrix(y_test, y_pred_knn_test)

"""## SVM"""

#importing support vector machine model from sklearn
from sklearn import svm
from sklearn.svm import SVC

"""### Kernel = 'linear'"""

model_svm_linear = SVC(C= 1, kernel = 'linear')
model_svm_linear.fit(X_train, y_train)

y_pred_svm_linear = model_svm_linear.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm_linear))

confusion_matrix(y_valid, y_pred_svm_linear)

"""### Kernel = 'rbf'

##### C = 1
"""

model_svm_rbf = svm.SVC(C = 1,kernel = 'rbf')
model_svm_rbf.fit(X_train, y_train)

y_pred_svm = model_svm_rbf.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm))

y_pred_svm = model_svm.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm))

"""##### C = 10"""

model_svm_rbf = svm.SVC(C = 10,kernel = 'rbf')
model_svm_rbf.fit(X_train, y_train)

y_pred_svm = model_svm_rbf.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm))

"""### Kernel = 'poly'

##### Degree = 2
"""

model_svm_poly = svm.SVC(C = 1,kernel = 'poly',degree=2)
model_svm_poly.fit(X_train, y_train)

y_pred_svm_poly = model_svm_poly.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm_poly))

"""##### Degree = 3"""

model_svm_poly = svm.SVC(C = 1,kernel = 'poly',degree=3)
model_svm_poly.fit(X_train, y_train)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm_poly))

"""##### Degree = 4"""

model_svm_poly = svm.SVC(C = 1,kernel = 'poly',degree=4)
model_svm_poly.fit(X_train, y_train)

y_pred_svm_poly = model_svm_poly.predict(X_valid)

from sklearn.metrics import classification_report
print(classification_report(y_valid, y_pred_svm_poly))

"""## Decision Tree

#### Criterion : Gini
"""

from sklearn.tree import DecisionTreeClassifier
classifier_dt=DecisionTreeClassifier() #by default gini 
classifier_dt.fit(X_train,y_train)

y_pred_dt_valid=classifier_dt.predict(X_valid)

print(classification_report(y_valid, y_pred_dt_valid))

confusion_matrix(y_valid, y_pred_dt_valid)

"""#### Criterion : Entropy"""

from sklearn.tree import DecisionTreeClassifier
classifier_dt_ent=DecisionTreeClassifier(criterion='entropy')
classifier_dt_ent.fit(X_train,y_train)

y_pred_dt_ent_valid=classifier_dt_ent.predict(X_valid)

print(classification_report(y_valid, y_pred_dt_ent_valid))

confusion_matrix(y_valid, y_pred_dt_ent_valid)

"""#### Criterion : Entropy, Splitter : Random"""

from sklearn.tree import DecisionTreeClassifier
classifier_dt_ent_rn=DecisionTreeClassifier(criterion='entropy', splitter='random')
classifier_dt_ent_rn.fit(X_train,y_train)

y_pred_dt_ent_rn_valid=classifier_dt_ent_rn.predict(X_valid)

print(classification_report(y_valid, y_pred_dt_ent_rn_valid))

confusion_matrix(y_valid, y_pred_dt_ent_rn_valid)

"""#### Test Set """

from sklearn.tree import DecisionTreeClassifier
classifier_dt_ent=DecisionTreeClassifier(criterion='entropy')
classifier_dt_ent.fit(X_train,y_train)

y_pred_dt_ent_test=classifier_dt_ent.predict(X_test)

print(classification_report(y_test, y_pred_dt_ent_test))

confusion_matrix(y_test, y_pred_dt_ent_test)

"""## Artificial Neural Network"""

import tensorflow as tf

ann = tf.keras.models.Sequential()

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

"""#### Test Set"""

y_pred_ann = ann.predict(X_test)
y_pred_ann = (y_pred_ann > 0.5)

from sklearn.metrics import accuracy_score
cm_ann = confusion_matrix(y_test, y_pred_ann)
print(cm_ann)
print('The accuracy of ANN Model is: ', round((accuracy_score(y_test, y_pred_ann)*100),3),"%")

"""### ANN - 2"""

ann = tf.keras.models.Sequential()

ann.add(tf.keras.layers.Dense(units=10, activation='relu'))

ann.add(tf.keras.layers.Dense(units=10, activation='relu'))

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

ann.fit(X_train, y_train, batch_size = 32, epochs = 200)

"""# Results"""

